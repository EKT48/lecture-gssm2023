{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6f6ed6-79e9-4efc-836a-040000ce8c89",
   "metadata": {
    "tags": []
   },
   "source": [
    "# day 2-3\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d9bc8c-8096-4d76-b983-250b7a4b8203",
   "metadata": {},
   "source": [
    "## 0. はじめに"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a794a09-51b9-449b-b699-32a6318de81b",
   "metadata": {},
   "source": [
    "ページ上部のメニューバーにある **Kernel** メニューをクリックし、プルダウンメニューから [**Change Kernel ...**] を選び、**gssm2023:Python** を選択してください。\n",
    "\n",
    "<img src=\"images/change_kernel1.png\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097064cc-d520-4893-a0e6-16ff5b34c15a",
   "metadata": {},
   "source": [
    "ノートブック上部の右隅に表示されたカーネル名が **gssm2023:Python** になっていることを確認してください。\n",
    "\n",
    "<img src=\"images/change_kernel2.png\" width=\"30%\">\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647bd5b1-64b2-4113-90c7-f31684ed2860",
   "metadata": {},
   "source": [
    "## 1. テキスト解析 (2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff63ae02-8b65-425f-958a-628f771494a8",
   "metadata": {},
   "source": [
    "### 1.0 事前準備 (関数の定義)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdce76ea-4a7a-44a6-a6a9-7556f9c1dc7c",
   "metadata": {},
   "source": [
    "以下のセルを実行してください"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda1c9f-8ab3-491d-9967-18d7cb3b7358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ワードクラウドを描画する\n",
    "def draw_wordcloud(word_counts_df):\n",
    "\n",
    "    font_path = !find ${HOME} -name \"ipaexg.ttf\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import wordcloud\n",
    "    %matplotlib inline\n",
    "\n",
    "    wc = wordcloud.WordCloud(\n",
    "        background_color='white',\n",
    "        font_path=font_path[0],\n",
    "        max_font_size=100)\n",
    "\n",
    "    img = wc.generate(' '.join(word_counts_df['表層'].tolist()))\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.imshow(img, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.tight_layout(pad=0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# トピックモデルによるワードクラウドを描画する\n",
    "def draw_topic_model(lda, feature_names, n_top_words=20):\n",
    "\n",
    "    font_path = !find ${HOME} -name \"ipaexg.ttf\"\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import wordcloud\n",
    "    %matplotlib inline\n",
    "\n",
    "    fig = plt.figure(figsize=(10,4))\n",
    "\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        sorted_text = ' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words-1:-1]])\n",
    "\n",
    "        wc = wordcloud.WordCloud(\n",
    "            background_color='white',\n",
    "            font_path=font_path[0],\n",
    "            max_font_size=100)\n",
    "\n",
    "        ax = fig.add_subplot(2, 3, topic_idx + 1)\n",
    "        img = wc.generate(sorted_text)\n",
    "        ax.imshow(img, interpolation='bilinear')\n",
    "        ax.set_title(f\"Topic # {topic_idx+1}:\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 共起ネットワークを描画する\n",
    "def plot_cooccur_network(df, word_counts, cutoff):\n",
    "\n",
    "    import networkx as nx\n",
    "    from networkx.algorithms import community\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "    %matplotlib inline\n",
    "\n",
    "    Xc = df.values\n",
    "    Xc_max = Xc.max()\n",
    "\n",
    "    words = df.columns\n",
    "    count_max = word_counts.max()\n",
    "\n",
    "    weights_w, weights_c = [], []\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "        if i < j and Xc[i,j] > cutoff:\n",
    "            weights_w.append((words[i], {'weight': word_counts[i] / count_max}))\n",
    "            weights_w.append((words[j], {'weight': word_counts[j] / count_max}))\n",
    "            weights_c.append((words[i], words[j], Xc[i,j] / Xc_max))\n",
    "\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(weights_w)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    # G = nx.minimum_spanning_tree(G)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # pos = nx.spring_layout(G, k=0.3)\n",
    "    pos = graphviz_layout(G, prog='neato', args='-Goverlap=\"scalexy\" -Gsep=\"+6\" -Gnodesep=0.8 -Gsplines=\"polyline\" -GpackMode=\"graph\" -Gstart={}'.format(43))\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "\n",
    "    communities = community.greedy_modularity_communities(G)\n",
    "    color_map = []\n",
    "    for node in G:\n",
    "        for i, c in enumerate(communities):\n",
    "            if node in c:\n",
    "                color_map.append(i)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=color_map, alpha=0.7, cmap=plt.cm.Set2, node_size=5000 * weights_n)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', edge_cmap=plt.cm.Blues, alpha=0.7, width=3 * weights_e)\n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 係り受けによる共起ネットワークを描画する\n",
    "def plot_dependency_network(df, word_counts, cutoff):\n",
    "\n",
    "    import networkx as nx\n",
    "    from networkx.algorithms import community\n",
    "    import matplotlib.pyplot as plt\n",
    "    import japanize_matplotlib\n",
    "    from networkx.drawing.nx_agraph import graphviz_layout\n",
    "    %matplotlib inline\n",
    "\n",
    "    Xc = df.values\n",
    "    Xc_max = Xc.max()\n",
    "\n",
    "    words = df.columns\n",
    "    count_max = word_counts.max()\n",
    "\n",
    "    weights_w, weights_c = [], []\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "        if Xc[i,j] > cutoff:\n",
    "            weights_w.append((words[i], {'weight': word_counts[i] / count_max}))\n",
    "            weights_w.append((words[j], {'weight': word_counts[j] / count_max}))\n",
    "            weights_c.append((words[i], words[j], Xc[i,j] / Xc_max))\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    G.add_nodes_from(weights_w)\n",
    "    G.add_weighted_edges_from(weights_c)\n",
    "    G.remove_nodes_from(list(nx.isolates(G)))\n",
    "    # G = nx.minimum_spanning_tree(G)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    # pos = nx.spring_layout(G, k=0.3)\n",
    "    pos = graphviz_layout(G, prog='neato', args='-Goverlap=\"scalexy\" -Gsep=\"+6\" -Gnodesep=0.8 -Gsplines=\"polyline\" -GpackMode=\"graph\" -Gstart={}'.format(43))\n",
    "    weights_n = np.array(list(nx.get_node_attributes(G, 'weight').values()))\n",
    "    weights_e = np.array(list(nx.get_edge_attributes(G, 'weight').values()))\n",
    "\n",
    "    communities = community.greedy_modularity_communities(G)\n",
    "    color_map = []\n",
    "    for node in G:\n",
    "        for i, c in enumerate(communities):\n",
    "            if node in c:\n",
    "                color_map.append(i)\n",
    "\n",
    "    nx.draw_networkx_nodes(G, pos, node_color=color_map, alpha=0.7, cmap=plt.cm.Set2, node_size=5000 * weights_n)\n",
    "    nx.draw_networkx_edges(G, pos, edge_color='gray', edge_cmap=plt.cm.Blues, alpha=0.7, width=3 * weights_e)\n",
    "    nx.draw_networkx_labels(G, pos, font_family='IPAexGothic')\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 対応分析の結果をプロットする\n",
    "def plot_coresp(df):\n",
    "\n",
    "    import numpy as np\n",
    "\n",
    "    table_N = df.values\n",
    "    row_sum = table_N.sum(axis=1)\n",
    "    col_sum = table_N.sum(axis=0)\n",
    "    n = df.values.sum()\n",
    "\n",
    "    expected = np.outer(row_sum, col_sum) / n\n",
    "    chisq = np.square(table_N - expected) / expected\n",
    "    residuals = (table_N - expected) / np.sqrt(expected)\n",
    "\n",
    "    # Standardized residuals\n",
    "    residuals = residuals / np.sqrt(n)\n",
    "\n",
    "    # Number of dimensions\n",
    "    nb_axes = min(residuals.shape[0]-1, residuals.shape[1]-1)\n",
    "\n",
    "    # Singular value decomposition\n",
    "    U, s, V = np.linalg.svd(residuals, full_matrices=True)\n",
    "    s = s[:nb_axes]\n",
    "    u = U[:, :nb_axes]\n",
    "    v = V.T[:, :nb_axes]\n",
    "\n",
    "    # row mass\n",
    "    row_mass = row_sum / n\n",
    "\n",
    "    # row coord = sv * u /sqrt(row.mass)\n",
    "    cc = (u * s)\n",
    "    row_coord = cc / np.sqrt(row_mass)[:, np.newaxis]\n",
    "\n",
    "    # col mass\n",
    "    col_mass = col_sum / n\n",
    "\n",
    "    # row coord = sv * v /sqrt(col.mass)\n",
    "    cc = (v * s)\n",
    "    col_coord = cc / np.sqrt(col_mass)[:, np.newaxis]\n",
    "\n",
    "    # eige nvalue\n",
    "    eige_nvalue = s ** 2\n",
    "\n",
    "    # contribution rate \n",
    "    contribution_rate = 100 * eige_nvalue / sum(eige_nvalue)\n",
    "    contribution_rate\n",
    "\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    # Plot of rows\n",
    "    plt.plot(row_coord[:, 0], row_coord[:, 1], 'o', color='red')\n",
    "    for i, label in enumerate(df.index):\n",
    "        plt.text(row_coord[i, 0], row_coord[i, 1], label, color='red', ha='left', va='bottom')\n",
    "\n",
    "    # Plot of columns\n",
    "    plt.plot(col_coord[:, 0], col_coord[:, 1], 'o', color='blue')\n",
    "    for i, label in enumerate(df.columns):\n",
    "        plt.text(col_coord[i, 0], col_coord[i, 1], label, color='blue', ha='left', va='bottom')\n",
    "\n",
    "    plt.axvline(0, linestyle='dashed', color='black')\n",
    "    plt.axhline(0, linestyle='dashed', color='black')\n",
    "    plt.xlabel(f\"Dim 1 ({contribution_rate[0]:.3f}%)\")\n",
    "    plt.ylabel(f\"Dim 2 ({contribution_rate[1]:.3f}%)\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 共起頻度行列を Jaccard 係数行列に変換する\n",
    "def jaccard_coef(cooccur_df):\n",
    "\n",
    "    Xc = cooccur_df.values\n",
    "    Xj = np.zeros(Xc.shape)\n",
    "    Xc_sum = cross_df.sum(axis=0).values\n",
    "\n",
    "    for i, j in zip(*Xc.nonzero()):\n",
    "        if i < j:\n",
    "            Xj[i,j] = Xc[i,j] / (Xc[i,j] + Xc_sum[i] + Xc_sum[j])\n",
    "\n",
    "    jaccard_df = pd.DataFrame(Xj, columns=cooccur_df.columns, index=cooccur_df.columns)\n",
    "\n",
    "    return jaccard_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffa2dec-da82-4b61-a10d-d00aad109f41",
   "metadata": {},
   "source": [
    "### 1.1 データのダウンロード (前回ダウンロード済みのためスキップ)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea1c51f-947e-49aa-a71f-9ab3185e06fa",
   "metadata": {},
   "source": [
    "以下のデータがダウンロード済みです"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c44667-eeb0-4acd-b5ea-5039e225129f",
   "metadata": {
    "tags": []
   },
   "source": [
    "| ファイル名 | 件数 | データセット | 備考 |\n",
    "| --- | --- | --- | --- |\n",
    "| rakuten-1000-2022-2023.xlsx.zip | 10,000 | •レジャー+ビジネスの 10エリア<br>•エリアごと 1,000件 (ランダムサンプリング)<br>•期間: 2022/1~2023 GW明け | 本講義の全体を通して使用する |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614d4536-de4f-4cd7-b6ce-e1ed5e60bd45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# もし、再度ダウンロードが必要な場合は残りの行のコメントマーク「#」を除去して、このセルを再実行してください\n",
    "\n",
    "# FILE_ID = \"1n-uvGoH7XQhxexN57hYXuFrkGeHKp-HV\"\n",
    "# !gdown --id {FILE_ID}\n",
    "# !unzip rakuten-1000-2022-2023.xlsx.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc9daa65-996b-4c16-bfc6-5e8de843291c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.2 データの読み込み (DataFrame型)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd612f-4a36-40ef-9685-c4898fafd177",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"rakuten-1000-2022-2023.xlsx\")\n",
    "print(df.shape)\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc5bee-fefd-4fe5-98e2-eee3d7965ab3",
   "metadata": {},
   "source": [
    "### 1.3 単語の抽出"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217a28a-284e-41a1-b981-ff59d3ed3a36",
   "metadata": {},
   "source": [
    "コメント列から単語を抽出する (単語を品詞「名詞」「形容詞」「未知語」で絞り込む)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2849575f-a94e-4805-b411-cdfc7d76dcc7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import MeCab\n",
    "\n",
    "tagger = MeCab.Tagger(\"-r ../tools/usr/local/etc/mecabrc --unk-feature 未知語\")\n",
    "\n",
    "word_counts = defaultdict(lambda: 1)\n",
    "words = []\n",
    "\n",
    "ZEN = \"\".join(chr(0xff01 + i) for i in range(94))\n",
    "HAN = \"\".join(chr(0x21 + i) for i in range(94))\n",
    "HAN2ZEN = str.maketrans(HAN, ZEN)\n",
    "\n",
    "# stopwords = ['する', 'ある', 'ない', 'いう', 'もの', 'こと', 'よう', 'なる', 'ほう']\n",
    "stopwords = [\"湯畑\"]\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    node = tagger.parseToNode(row[\"コメント\"].translate(HAN2ZEN))\n",
    "    while node:\n",
    "        features = node.feature.split(',')\n",
    "        pos1 = features[0]\n",
    "        pos2 = features[1] if len(features) > 1 else \"\"\n",
    "        base = features[6] if len(features) > 6 else None\n",
    "\n",
    "        if base not in stopwords:\n",
    "\n",
    "            if (pos1 == \"名詞\" and pos2 == \"一般\"):\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"名詞\"\n",
    "                key = (base, postag)\n",
    "                word_counts[key] += 1\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "            elif (pos1 == \"名詞\" and pos2 == \"形容動詞語幹\"):\n",
    "                base = base if base is not None else node.surface\n",
    "                base = f\"{base}だ\"\n",
    "                postag = \"形容動詞\"\n",
    "                key = (base, postag)\n",
    "                word_counts[key] += 1\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "            elif pos1 == \"形容詞\":\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"形容詞\"\n",
    "                key = (base, postag)\n",
    "                word_counts[key] += 1\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "            elif pos1 == \"未知語\":\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"未知語\"\n",
    "                key = (base, postag)\n",
    "                word_counts[key] += 1\n",
    "                words.append([index + 1, base, postag, row[\"カテゴリー\"], row[\"エリア\"], key])\n",
    "\n",
    "        node = node.next\n",
    "\n",
    "columns = [\n",
    "    \"文書ID\",\n",
    "    # \"単語ID\",\n",
    "    \"表層\",\n",
    "    \"品詞\",\n",
    "    \"カテゴリー\",\n",
    "    \"エリア\",\n",
    "    \"dict_key\",\n",
    "]\n",
    "docs_df = pd.DataFrame(words, columns=columns)\n",
    "print(docs_df.shape)\n",
    "display(docs_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a591f4-512a-41c4-ae83-b9014096dd09",
   "metadata": {},
   "source": [
    "### 1.4 単語の出現頻度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a03c6f-1a67-4e96-a78f-ec471d4098d5",
   "metadata": {},
   "source": [
    "単語の出現頻度をカウントする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5bed21-5e42-4f00-8bc9-32ed5b0a5a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for i, (k, v) in enumerate(sorted(word_counts.items(), key=lambda x:x[1], reverse=True)):\n",
    "    word_list.append((i, k[0], v, k))\n",
    "\n",
    "columns = [\n",
    "    \"単語ID\",\n",
    "    \"表層\",\n",
    "    \"出現頻度\",\n",
    "    \"dict_key\"\n",
    "]\n",
    "word_counts_df = pd.DataFrame(word_list, columns=columns)[0:75]\n",
    "print(word_counts_df.shape)\n",
    "display(word_counts_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9127684b-da31-4a70-ab00-8900f3ffed27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(docs_df, word_counts_df, how=\"inner\", on=\"dict_key\", suffixes=[\"\", \"_right\"])\n",
    "docs_df = merged_df[[\"文書ID\", \"単語ID\", \"表層\", \"品詞\", \"カテゴリー\", \"エリア\", \"dict_key\"]]\n",
    "print(docs_df.shape)\n",
    "display(docs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec122799-b82c-4916-aff6-40db98deb874",
   "metadata": {},
   "source": [
    "### 1.5 ワードクラウド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538108f3-34fb-4b05-a120-60623c54bfc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_wordcloud(word_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2cc2c3c-3383-4e49-b678-471d3f7a530a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.6 文書-単語表の作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fadd536-1426-4c17-ab7e-2b7156116d5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_df = pd.crosstab([docs_df['カテゴリー'], docs_df['エリア'], docs_df['文書ID']], docs_df['単語ID'], margins=False)\n",
    "cross_df.columns = word_counts_df[\"表層\"]\n",
    "print(cross_df.shape)\n",
    "display(cross_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11786769-22aa-44e3-a817-2a593497f00f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.7 共起ネットワーク図"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48c9db-bbed-43a8-8451-4709abebd87a",
   "metadata": {},
   "source": [
    "#### 1.7.1 共起度行列を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c37515-b171-446a-9020-8f6e1391facf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "X = csc_matrix(cross_df.values)\n",
    "X[X > 0] = 1\n",
    "Xc = (X.T * X)\n",
    "Xc = np.triu(Xc.toarray())\n",
    "\n",
    "cooccur_df = pd.DataFrame(Xc, columns=cross_df.columns, index=cross_df.columns)\n",
    "print(cooccur_df.shape)\n",
    "display(cooccur_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ed95a2-60f0-4248-948d-1a77f63136ee",
   "metadata": {},
   "source": [
    "#### 1.7.2 Jaccard 係数を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d053b-5d08-4aea-9052-f4909bec75b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "jaccard_df = jaccard_coef(cooccur_df)\n",
    "print(jaccard_df.shape)\n",
    "display(jaccard_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cfb73da-5154-41a5-ad24-3f76038be128",
   "metadata": {},
   "source": [
    "#### 1.7.3 プロットする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d236a1f2-2cb2-4dba-8da3-dee524f24b13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_counts = cross_df.sum(axis=0).values\n",
    "plot_cooccur_network(jaccard_df, word_counts, np.sort(jaccard_df.values.reshape(-1))[::-1][60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133def7e-50ce-4166-b8a5-8717df4f5609",
   "metadata": {},
   "source": [
    "### 1.8 係り受けネットワーク図 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae862d88-43fb-49f9-a767-1b03249ac680",
   "metadata": {},
   "source": [
    "#### 1.8.1 係り受け行列を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29bcb28-c74d-4059-9f8b-fa8aa851d528",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# チャンク(chunk)から単語を取り出す\n",
    "def get_words(tree, from_chunk, stopwords):\n",
    "    beg = from_chunk.token_pos\n",
    "    end = from_chunk.token_pos + from_chunk.token_size\n",
    "\n",
    "    words = []\n",
    "    for i in range(beg, end):\n",
    "        token = tree.token(i)\n",
    "        features = token.feature.split(',')\n",
    "        pos1 = features[0]\n",
    "        pos2 = features[1] if len(features) > 1 else \"\"\n",
    "        base = features[6] if len(features) > 6 else None\n",
    "\n",
    "        if base not in stopwords:\n",
    "\n",
    "            if (pos1 == \"名詞\" and pos2 == \"一般\"):\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"名詞\"\n",
    "                key = (base, postag)\n",
    "                words.append(key)\n",
    "\n",
    "            elif (pos1 == \"名詞\" and pos2 == \"形容動詞語幹\"):\n",
    "                base = base if base is not None else node.surface\n",
    "                base = f\"{base}だ\"\n",
    "                postag = \"形容動詞\"\n",
    "                key = (base, postag)\n",
    "                words.append(key)\n",
    "\n",
    "            elif pos1 == \"形容詞\":\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"形容詞\"\n",
    "                key = (base, postag)\n",
    "                words.append(key)\n",
    "\n",
    "            elif pos1 == \"未知語\":\n",
    "                base = base if base is not None else node.surface\n",
    "                postag = \"未知語\"\n",
    "                key = (base, postag)\n",
    "                words.append(key)\n",
    "\n",
    "    return words\n",
    "\n",
    "\n",
    "import CaboCha\n",
    "\n",
    "cp = CaboCha.Parser(\"-r ../tools/usr/local/etc/cabocharc\")\n",
    "\n",
    "ZEN = \"\".join(chr(0xff01 + i) for i in range(94))\n",
    "HAN = \"\".join(chr(0x21 + i) for i in range(94))\n",
    "HAN2ZEN = str.maketrans(HAN, ZEN)\n",
    "\n",
    "# stopwords = ['する', 'ある', 'ない', 'いう', 'もの', 'こと', 'よう', 'なる', 'ほう']\n",
    "stopwords = ['*']\n",
    "\n",
    "pair_counts = defaultdict(lambda: 1)\n",
    "pairs = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # print(cp.parseToString(row[\"コメント\"].translate(HAN2ZEN)))\n",
    "    tree = cp.parse(row[\"コメント\"].translate(HAN2ZEN))\n",
    "\n",
    "    # chunks = get_chunks(tree)\n",
    "    chunks = {}\n",
    "    key = 0\n",
    "    for i in range(tree.size()):\n",
    "        tok = tree.token(i)\n",
    "        if tok.chunk:\n",
    "            chunks[key] = tok.chunk\n",
    "            key += 1\n",
    "\n",
    "    for from_chunk in chunks.values():\n",
    "        if from_chunk.link < 0:\n",
    "            continue\n",
    "        to_chunk = chunks[from_chunk.link]\n",
    "\n",
    "        # from_surface = get_surface(tree, from_chunk)\n",
    "        from_words = get_words(tree, from_chunk, stopwords)\n",
    "\n",
    "        # to_surface = get_surface(tree, to_chunk)\n",
    "        to_words = get_words(tree, to_chunk, stopwords)\n",
    "\n",
    "    for f in from_words:\n",
    "        for t in to_words:\n",
    "            key = (f[0], t[0])\n",
    "            pair_counts[key] += 1\n",
    "\n",
    "Xc = cooccur_df.values\n",
    "Xd = np.zeros(Xc.shape)\n",
    "\n",
    "for (f,t), v in pair_counts.items():\n",
    "    columns = list(cooccur_df.columns)\n",
    "    if f in columns and t in columns:\n",
    "        i = columns.index(f)\n",
    "        j = columns.index(t)\n",
    "        Xd[i,j] = v\n",
    "\n",
    "dependency_df = pd.DataFrame(Xd, columns=cooccur_df.columns, index=cooccur_df.columns)\n",
    "print(dependency_df.shape)\n",
    "display(dependency_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3c0686-8812-4299-b0d6-09c8c5a91547",
   "metadata": {},
   "source": [
    "#### 1.8.2 条件付き確率を求める"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a66387-7882-4204-aec0-4ae0a69a5f5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Xc = cooccur_df.values\n",
    "Xd = np.zeros(Xc.shape)\n",
    "word_counts = cross_df.sum(axis=0).values\n",
    "\n",
    "for (f,t), v in pair_counts.items():\n",
    "    columns = list(cooccur_df.columns)\n",
    "    if f in columns and t in columns:\n",
    "        i = columns.index(f)\n",
    "        j = columns.index(t)\n",
    "        Xd[i,j] = v / word_counts[i]\n",
    "\n",
    "dependency_df = pd.DataFrame(Xd, columns=cooccur_df.columns, index=cooccur_df.columns)\n",
    "print(dependency_df.shape)\n",
    "display(dependency_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeadfbfc-a59d-4521-9a83-df8d151ad94d",
   "metadata": {},
   "source": [
    "#### 1.8.3 プロットする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833e01ec-a6fe-4aa0-9a45-76f3048711d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "word_counts = cross_df.sum(axis=0).values\n",
    "plot_dependency_network(dependency_df, word_counts, np.sort(dependency_df.values.reshape(-1))[::-1][60])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0e4380-5010-4c7b-bfa6-a7d31d33cc9c",
   "metadata": {},
   "source": [
    "### 1.9 対応分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d3655d-0de3-4261-a75b-683924eb3958",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cross_df.shape)\n",
    "display(cross_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df75d709-9858-44fe-8c3e-848096449075",
   "metadata": {},
   "source": [
    "#### 1.9.1 クロス集計表を作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f701946c-4d70-447e-8ef3-9fc2bff33e6d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "category_cross_df = cross_df.groupby(level='カテゴリー').sum()\n",
    "area_cross_df = cross_df.groupby(level='エリア').sum()\n",
    "aggregate_df = pd.concat([category_cross_df, area_cross_df])\n",
    "print(aggregate_df.shape)\n",
    "display(aggregate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe18698-0c76-4da1-8279-c720716a401b",
   "metadata": {},
   "source": [
    "#### 1.9.2 対応分析プロットを作成する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e43b3d-7ac7-483b-b0d0-b52e45b48af3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_coresp(aggregate_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c196efc-b548-4be5-ae0e-2784d26fa6c5",
   "metadata": {},
   "source": [
    "### 1.10 トピックモデル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc745ac6-145f-4890-8eb0-d1ed04e7b145",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(cross_df.shape)\n",
    "display(cross_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9839a-a004-47a9-958e-c26c92b9cefc",
   "metadata": {},
   "source": [
    "#### 1.10.1 トピックを抽出する (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ff4904-3dbe-4637-900e-bc0365a159e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "\n",
    "lda = LDA(max_iter=25, learning_method='batch', random_state=0, n_jobs=-1, n_components=6)\n",
    "lda.fit(cross_df.values)\n",
    "\n",
    "n_top_words = 20\n",
    "feature_names = cross_df.columns\n",
    "\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic # {topic_idx+1}:\", end=\" \")\n",
    "    for i in topic.argsort()[:-n_top_words-1:-1]:\n",
    "        print(feature_names[i], end=\" \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e0b8e1-6d13-403c-8fc0-2b474ee1a830",
   "metadata": {},
   "source": [
    "#### 1.10.2 結果をワードクラウドで描画する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0964af57-f372-44fb-9b37-6e5cdd555115",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_top_words = 100\n",
    "draw_topic_model(lda, feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40790a01-51e1-48df-8ecb-a9ed4c89d070",
   "metadata": {},
   "source": [
    "## 2. エリアごとで頻出単語を比較する"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aea0bd7-4b3b-46ad-a35d-729e1da1bd66",
   "metadata": {},
   "source": [
    "### 2.1 レジャー"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5707ce-52a3-4c9a-b0bf-a635f6e26b3e",
   "metadata": {},
   "source": [
    "#### 2.1.1 単語の出現頻度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d155918-4846-4bc2-83bb-663c58200061",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_A_df = docs_df[docs_df['カテゴリー'] == \"A_レジャー\"]\n",
    "word_counts_A_df = pd.DataFrame(docs_A_df[\"単語ID\"].value_counts()).reset_index()\n",
    "word_counts_A_df.columns = [\"単語ID\", \"出現頻度\"]\n",
    "merged_df = pd.merge(word_counts_A_df, word_counts_df, how=\"inner\", on=\"単語ID\", suffixes=[\"\", \"_right\"])\n",
    "word_counts_A_df = merged_df[[\"単語ID\", \"表層\", \"出現頻度\", \"dict_key\"]][0:75]\n",
    "print(word_counts_A_df.shape)\n",
    "display(word_counts_A_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b09df75-4ee9-485d-9d46-8b7bcc5a1080",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(docs_A_df, word_counts_A_df, how=\"inner\", on=\"dict_key\", suffixes=[\"\", \"_right\"])\n",
    "docs_A_df = merged_df[[\"文書ID\", \"単語ID\", \"表層\", \"品詞\", \"カテゴリー\", \"エリア\", \"dict_key\"]]\n",
    "print(docs_A_df.shape)\n",
    "display(docs_A_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d5087d-66b0-4334-bf3c-8a38c307b43c",
   "metadata": {},
   "source": [
    "#### 2.1.2 ワードクラウド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf107e49-7c33-44bb-80ee-b12871c31f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_wordcloud(word_counts_A_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3802bef0-2817-4806-a000-d53098f78495",
   "metadata": {},
   "source": [
    "### 2.2 ビジネス"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8f70bb-8926-4dc8-a5cf-b4d0ef10191e",
   "metadata": {},
   "source": [
    "#### 2.2.1 単語の出現頻度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c1de0f-d356-4569-af00-579b97958693",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "docs_B_df = docs_df[docs_df['カテゴリー'] == \"B_ビジネス\"]\n",
    "word_counts_B_df = pd.DataFrame(docs_B_df[\"単語ID\"].value_counts()).reset_index()\n",
    "word_counts_B_df.columns = [\"単語ID\", \"出現頻度\"]\n",
    "merged_df = pd.merge(word_counts_B_df, word_counts_df, how=\"inner\", on=\"単語ID\", suffixes=[\"\", \"_right\"])\n",
    "word_counts_B_df = merged_df[[\"単語ID\", \"表層\", \"出現頻度\", \"dict_key\"]][0:75]\n",
    "print(word_counts_B_df.shape)\n",
    "display(word_counts_B_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce966aa-9a7e-4e0f-b253-807ca16cccac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merged_df = pd.merge(docs_B_df, word_counts_B_df, how=\"inner\", on=\"dict_key\", suffixes=[\"\", \"_right\"])\n",
    "docs_B_df = merged_df[[\"文書ID\", \"単語ID\", \"表層\", \"品詞\", \"カテゴリー\", \"エリア\", \"dict_key\"]]\n",
    "print(docs_B_df.shape)\n",
    "display(docs_B_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629918d4-9279-4180-b64e-12e4eb37d72b",
   "metadata": {},
   "source": [
    "#### 2.1.2 ワードクラウド"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92894314-404a-4e8c-945f-f22403812c2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "draw_wordcloud(word_counts_B_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gssm2023:Python",
   "language": "python",
   "name": "conda-env-gssm2023-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
